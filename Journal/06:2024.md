# Notes

# Week 1
### Todo
- [ ] 

### Questions to ask
- 

### Done 
- 

# Week 2
### Todo
- [ ] 

### Questions to ask
- 

### Done 
- 

# Week 3
### Todo
- [ ] Orchestrator feature to enable processing Kafka message queue
- [ ] Kakfa check if message 
- [ ] New Kafka message queue (Producer) and content format. 
- [ ] New Kafka message queue (Consumer) that process all recent pending applicaiton
- [ ] New API for added PartnerID as new parameter along side with campaignID, when it becomes active, type (reprocessing)
  - [ ] Class of 1 ProgramID (long) and list of associated partnerIDs (long)
- [ ] IndexController - add a new requestType

### Questions to ask
- 

### Done 
- 

# Week 4
### Todo
- [ ] 

### Questions to ask
- 

### Done 
- 


package com.impact.partnerships.imports;

import com.google.cloud.functions.Context;
import com.google.cloud.storage.contrib.nio.CloudStorageFileSystem;
import com.google.gson.Gson;
import com.google.gson.JsonObject;
import com.mediarails.core.util.FileUtils;
import com.mediarails.core.util.rollbar.Rollbar;
import com.mediarails.core.util.rollbar.Rollbar.RollbarLogLevel;
import com.mediarails.database.mysql.MySqlClient;
import com.mediarails.gcp.ProjectEnvironment;
import com.mediarails.gcp.secrets.SecretUtils;
import com.typesafe.config.Config;
import org.apache.commons.lang3.ArrayUtils;
import org.json.JSONObject;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.io.*;
import java.net.URI;
import java.net.URLEncoder;
import java.nio.charset.StandardCharsets;
import java.nio.file.FileSystem;
import java.nio.file.*;
import java.sql.SQLException;
import java.time.Duration;
import java.time.Instant;
import java.util.Arrays;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

public class CloudStorageFile {

    private MySqlClient mySql;
    private final boolean REUSE_SQL_CONNECTION = false;
    private static ProjectEnvironment projectEnv;
    private static Config appCfg;
    
    final static String PAUSE_IMPORT_LOAD_FILE = "pause_import_load";
    final static String PAUSE_IMPORT_PROCESS_FILE = "pause_import_process";
    final static String SUPPORTED_IMPORT_FOLDERS = "vertical_recommendations|campaign_recommendations";
    final static String[] TAB_DELIMITER_IMPORT_FOLDERS = new String[]{"vertical_recommendations", "campaign_recommendations"};
    private static final String TEMP_FOLDER = "/tmp"; // The only available local Function folder
    private static final Logger log = LoggerFactory.getLogger(CloudStorageFile.class);

    private final String importFile;
    private final String importFileName;
    private final String importFolder;
    private final String importBucket;
    private final String bucketFileUri;
    private final String importLevel;
    private final char columnDelimiter;
    private final ImportType importType;
    private final String importTable;
    private final String project;
    private boolean performImport = true;
    private boolean performLoadToDb = true;
    private int jobId = 0;

    public MySqlClient getMySql() {
        return mySql;
    }

    public CloudStorageFile(CloudStorageEvent event, MySqlClient mySqlClient,
            ProjectEnvironment env, Config appCfg) throws UnsupportedEncodingException {
        projectEnv = env;
        this.mySql = mySqlClient;
        this.importFile = event.name; // Typically prefixed with folder name, so 'vertical_recommendations/file.csv'
        this.importFileName = getImportFileName(this.importFile);
        this.importLevel = this.importFile.toLowerCase().contains("(incremental)") ? "incremental" : "full";
        this.importFolder = getImportFolder(importFile);
        this.importType = getImportType(importFolder);
        this.importBucket = event.bucket;
        this.columnDelimiter = ArrayUtils.contains(TAB_DELIMITER_IMPORT_FOLDERS, importFolder) ? '\t' : ',';
        this.bucketFileUri = String.format("gs://%s/%s", importBucket, getEncodedName(importFile));
        this.importTable = getImportTableName();
        this.project = projectEnv.getProjectId();
//        this.project = projectEnv.getProjectId();
        this.project = "impact-dev-partnerships";
        CloudStorageFile.appCfg = appCfg;
    }

    public void process() throws Exception {

        // NOTE: Import files are archived by bucket retention policy (400 days).
        performLoadToDb = !fileExistsInImportBucket(PAUSE_IMPORT_LOAD_FILE);
        performImport = !fileExistsInImportBucket(PAUSE_IMPORT_PROCESS_FILE);

        if (!performLoadToDb) {
            log.warn(String.format("WARNING: Not loading file into import table as '%s' is present in the directory",
                    PAUSE_IMPORT_LOAD_FILE));
            return;
        }

        log.info(String.format("Importing records into table '%s' ...", importTable));
        createSqlConnection(REUSE_SQL_CONNECTION);

        try {
            jobId = LogHelper.createPendingJob(importType, bucketFileUri, mySql);
            if (loadToDb()) {
                // File is loaded. Run SQL stored procedure to process the imported rows.
                processImportedRecords();
            }
        } catch (Exception ex) {
            LogHelper.logException(jobId, importType, bucketFileUri, ex, mySql);
            try (Rollbar rollbar = new Rollbar()) {
                rollbar.record(RollbarLogLevel.ERROR, ex);
            }
        }
    }
    
    private boolean loadToDb() throws SQLException, IOException {
        
        int rowsInTable = -1;
        List<String> importColumns = this.getImportColumns();
        List<String> supportedImportColumns = getSupportedImportColumns(importFolder);
        
        if (importColumns.size() != supportedImportColumns.size() || !importColumns.containsAll(supportedImportColumns)) {
            log.error("ERROR: Actual column headers don't match supported headers. Aborting import.");
            log.error(String.format("Detected %d column(s) using delimiter '%s': %s", importColumns.size(), columnDelimiter, importColumns));
            
            return false;
        }

        Instant ImportStart = Instant.now();
        Path localTempfilePath = Path.of(TEMP_FOLDER, importFileName);
        
        try (
                // Using "NIO Filesystem Provider for Google Cloud Storage"
                // to read file from GCP bucket.
                CloudStorageFileSystem fs = CloudStorageFileSystem.forBucket(importBucket);
                InputStream inputStream = Files.newInputStream(fs.getPath(importFile))
        ) {
            // Copy file from GCP Bucket to local tmp directory.
            // TODO: Preserve bucket folder locally? For now, simultaneous imports (files) must be unique within function instance.
            log.debug(String.format("Copying file '%s' from GCP bucket to local folder.", importFile));
            long bytesCopied = Files.copy(inputStream, localTempfilePath, StandardCopyOption.REPLACE_EXISTING);
            log.debug(String.format("Done. Copied %d bytes.", bytesCopied));
            
            // Bulk load data from local temp file to MySql import table.
            // TODO: Stream data from bucket to table directly (using setLocalInfileInputStream)?
            // TODO: Allow getBulkLoadCommand to take delimiter param.
            String sql = getBulkLoadCommand(importType, localTempfilePath.toString(), importTable, "utf8mb4");
            log.debug("Loading data into import staging table...");
            mySql.execute(sql);
            log.debug("Done loading data into import staging table.");
            rowsInTable = mySql.selectInt(String.format("SELECT COUNT(*) FROM %s", importTable));

            Instant importEnd = Instant.now();
            Duration elapsed = Duration.between(ImportStart, importEnd);

            LogHelper.markLoaded(jobId, false, mySql);
            log.info(
                    String.format(
                            "Imported records into table '%s'. Total records in table: %d. Import execution (sec): %s.",
                            importTable, rowsInTable, elapsed.toMillis() / 1000));
            return true;
        } finally {
            // Delete bucket file copied locally.
            Files.deleteIfExists(localTempfilePath);
        }
    }

    public static String getBulkLoadCommand(
            ImportType importType,
            String bulkloadFile,
            String tableName,
            String mysqlCharacterSet) {

        String bulkloadFileName;
        switch (importType) {
            case PARTNER_RECOMMENDATION:
            case CAMPAIGN_RECOMMENDATION:
                bulkloadFileName = "BulkDataCommandCampaignRec.sql";
                break;
            case VERTICAL_RECOMMENDATION:
                bulkloadFileName = "BulkDataCommandVerticalRec.sql";
                break;
            default:
                bulkloadFileName = "BulkDataCommandGeneric.sql";
        }
        
        String bulkloadCmdScript = FileUtils.getFileAsStringFromResources(bulkloadFileName);
        String bulkloadCmd = String.format(bulkloadCmdScript, bulkloadFile, tableName, mysqlCharacterSet);
        return bulkloadCmd;
    }
    
    public void processImportedRecords() throws Exception {

        if (!performImport) {
            log.warn(String.format(
                    "WARNING: Not processing any rows in import table as file '%s' is present in the directory",
                    PAUSE_IMPORT_PROCESS_FILE));
            LogHelper.markLoaded(jobId, true, mySql);
            return;
        }

        JSONObject sqlParams = new JSONObject();

        if (importLevel == null || !importLevel.toLowerCase().matches("full|incremental")) {
            throw new UnsupportedOperationException(String.format("Unsupported import type '%s'", importLevel));
        }

        log.info(String.format("Processing staged import records in table '%s' for feed type '%s'", importTable, importLevel));
        
        sqlParams.put("importLevel", importLevel);
        sqlParams.put("jobId", this.jobId);
        sqlParams.put("uri", bucketFileUri);
        
        String result = executeQuery(String.format("CALL import.process_%s('%s');", importFolder, sqlParams));
        log.info(String.format("Result: %s", result));
    }

    // Gets the GCP bucket file's header rows to determine column order
    public static String[] getHeaderColumns(String importBucket, String importFile, char columnDelimiter) throws IOException {

        try (
                CloudStorageFileSystem fs = CloudStorageFileSystem.forBucket(importBucket);
                InputStream inputStream = Files.newInputStream(fs.getPath(importFile));
                BufferedReader bufferReader = new BufferedReader(new InputStreamReader(inputStream, StandardCharsets.UTF_8))
        ) {
            return readHeaderColumns(bufferReader, columnDelimiter);
        }
    }

    public static String[] getHeaderColumns(String bucketUri, char columnDelimiter)
            throws IOException {

        try (
                InputStream inputStream = Files.newInputStream(Paths.get(URI.create(bucketUri)));
                BufferedReader bufferReader = new BufferedReader(new InputStreamReader(inputStream, StandardCharsets.UTF_8))
        ) {
            return readHeaderColumns(bufferReader, columnDelimiter);
        }
    }

    private static String[] readHeaderColumns(BufferedReader bufferReader, char columnDelimiter) throws IOException {
        FunctionHelper.skipBOM(bufferReader);
        String headerRow = bufferReader.readLine();
        return headerRow.split(Character.toString(columnDelimiter));
    }

    // List of expected/supported columns for each import file.
    // This is used to block processing if the columns don't match to avoid
    // accidentally processing a bad file and making changes to live data in the db.
    public static List<String> getSupportedImportColumns(String importFolder) {

        Map<String, String[]> tableColumns = new HashMap<>();
        tableColumns.put("vertical_recommendations", new String[]{
                "match_type",
                "parent_category",
                "child_category",
                "partner_id",
                "ranking",
                "opted_in_for_autojoin",
                "is_safe_publisher",
                "partner_category_exclusions",
                "minor_country_iso_code_alignment",
                "major_country_iso_code_alignment",
                "product_editions",
                "personas",
                "business_model"
        });

                // Associated with BulkDataCommandPartnerRec.sql
                // Associated with BulkDataCommandCampaignRec.sql
        tableColumns.put("campaign_recommendations", new String[]{
                "brand_id",
                "listing_id",
                "partner_id",
                "reason_code",
                "status",
                "influencer_partners_you_will_love",
                "partners_you_will_love",
                "is_high_aov",
                "is_high_revenue",
                "similar_products",
                "pending_contract",
                "is_introducer",
                "is_influencer",
                "is_closer",
                "top_partner_for_upcoming_holiday",
                "partner_works_with_similar_brands",
                "is_instagram",
        });

        return Arrays.asList(tableColumns.get(importFolder));
    }

    public static String getEncodedName(String fileName)
            throws UnsupportedEncodingException {

        String encodedFileName = URLEncoder.encode(fileName, StandardCharsets.UTF_8);

        // Library used to interact with GCP buckets doesn't like having spaces encoded as '+'
        return encodedFileName.replace("+", "%20");
    }

    public static ImportType getImportType(String importFolder) {
        Map<String, ImportType> folderToImportType = new HashMap<>();
        folderToImportType.put("vertical_recommendations", ImportType.VERTICAL_RECOMMENDATION);
        folderToImportType.put("campaign_recommendations", ImportType.PARTNER_RECOMMENDATION);
        folderToImportType.put("campaign_recommendations", ImportType.CAMPAIGN_RECOMMENDATION);

        ImportType importType = folderToImportType.get(importFolder);
        if (importType == null) {
            throw new UnsupportedOperationException(
                    String.format("Unable to map folder '%s' to known import type", importFolder));
        }
        return importType;
    }

    public static String getImportFolder(String fileName) {
        int index = fileName.indexOf('/');
        return fileName.substring(0, index == -1 ? 0 : index);
    }

    public static String getImportFileName(String importFile) {
        if (importFile != null && importFile.length() > 0) {
            int index = importFile.lastIndexOf('/');
            return importFile.substring(index == -1 ? 0 : index + 1);
        }
        return importFile;
    }

    private String getImportTableName() {
        Map<String, String> folderToTableMappings = new HashMap<String, String>();
        folderToTableMappings.put("vertical_recommendations", "import.i_vertical_recommendation");
        folderToTableMappings.put("campaign_recommendations", "import.i_partner_recommendation");
        folderToTableMappings.put("campaign_recommendations", "import.i_campaign_recommendation");

        String tableName = folderToTableMappings.get(importFolder);
        if (tableName == null) {
            throw new UnsupportedOperationException(
                    String.format("Importing files from folder '%s' is not supported", importFolder));
        }
        return tableName;
    }

    private boolean fileExistsInImportBucket(String fileName)
            throws IOException {

        String fullPath = String.format("%s/%s", importFolder, fileName);
        try (FileSystem fs = CloudStorageFileSystem.forBucket(importBucket)) {
            for (Path path : Files.newDirectoryStream(fs.getPath(importFolder))) {
                if (path.toString().equalsIgnoreCase(fullPath)) {
                    return true;
                }
            }
        }

        return false;
    }

    protected static boolean validStorageObjectEvent(CloudStorageEvent event, Context context,
            String expectedImportBucket) {

        Boolean passedValidation = true;

        // Validate bucket name
        if (!event.bucket.equalsIgnoreCase(expectedImportBucket)) {
            log.warn(String.format("Unsupported bucket '%s'. Only '%s' is accepted",
                            event.bucket,
                            expectedImportBucket));
            passedValidation = false;
        }

        // Validate event type
        String eventType = context.eventType();
        if (!eventType.equalsIgnoreCase("google.storage.object.finalize")) {
            log.warn(String.format("Unsupported event type '%s'. Only 'finalize' is accepted",
                            eventType));
            passedValidation = false;
        }

        // Validate folder name
        if (!getImportFolder(event.name).toLowerCase().matches(SUPPORTED_IMPORT_FOLDERS)) {
            log.warn("Ignoring event as folder is not part of those targeted for import processing.");
            passedValidation = false;
        }

        // Validate file name
        if (event.name.toLowerCase().endsWith(PAUSE_IMPORT_LOAD_FILE)
                || event.name.toLowerCase().endsWith(PAUSE_IMPORT_PROCESS_FILE)) {
            log.info(String.format("Triggered event is for file '%s' which is on the ignore list. Skipping processing",
                            event.name));
            passedValidation = false;
        }

        return passedValidation;
    }

    // Returns a list of columns to be imported. Depending on the data source,
    // column order may be non-deterministic, so the header row of the
    // report file is read each time to determine order.
    // NOTE: *All* fields in the import file must be specified, but all columns
    // in the destination table do not need to be present in the file.
    // The field names specified here, are the names of the columns in the destination table.
    // The order used here links them to the corresponding fields in the import file.
    public List<String> getImportColumns() throws IOException {

        String[] headerColumns = getHeaderColumns(importBucket, importFile, columnDelimiter);
        return Arrays.asList(headerColumns);
    }

    // Executes the given SQL query
    protected String executeQuery(String query) throws Exception {
        try {
            createSqlConnection(REUSE_SQL_CONNECTION);
            log.debug("Running query: {}", query);
            return mySql.selectString(query);
        } catch (Exception ex) {
            log.error("ERROR: Exception querying SQL: " + ex);
            try (Rollbar rollbar = new Rollbar()) {
                rollbar.record(RollbarLogLevel.ERROR, ex);
            }

            throw ex;
        } finally {
            if (!REUSE_SQL_CONNECTION && mySql != null) {
                try {
                    mySql.close();
                } catch (SQLException ex) {
                    log.error("ERROR: Exception closing SQL connection: " + ex);
                    try (Rollbar rollbar = new Rollbar()) {
                        rollbar.record(RollbarLogLevel.ERROR, ex);
                    }
                    //Todo: Do we remove the throw statement in finally block?
                    throw ex;
                }
            }
        }
    }

    // Returns a valid SQL connection, either a new or existing connection
    private void createSqlConnection(boolean reuseConnection) throws Exception {
        
        final int TIMEOUT_SEC = 5;
        log.debug(String.format("Preparing SQL connection in %s (attempt reuse: %s)", FunctionHelper.getEffectiveEnvironment(projectEnv), reuseConnection));

        if (!reuseConnection || mySql == null || !mySql.checkOutConnection().isValid(TIMEOUT_SEC)) {
            log.debug("Establishing new SQL connection");
            
            String connectionInfoSecretName = appCfg.getString("gcp.database.connectioninfo.secret.name");
            JsonObject secret = getDbSecret(connectionInfoSecretName);
            String port = secret.has("port") ? ":" + secret.get("port").getAsString() :"";

            String url = String.format("jdbc:mariadb://%s%s/%s?allowMultiQueries=true&allowLoadLocalInfile=true",
                    secret.get("server").getAsString(),
                    port,
                    secret.get("database").getAsString());

            // Connection is intentionally not closed as we want to reuse it whenever possible (unless reuseConnection is false)
            log.debug(String.format("Connecting to SQL instance '%s'", secret.get("server").getAsString()));
            mySql = new MySqlClient(url, secret.get("user").getAsString(), secret.get("password").getAsString());
            log.debug("Successfully connected to SQL");
        } else {
            log.debug("Reusing existing SQL connection");
        }
    }

    private static JsonObject getDbSecret(String datasourceSecretName) throws java.io.IOException {
        log.debug("Getting database secret for {}", datasourceSecretName);

        JsonObject secret;
        Instant secretBegin = Instant.now();
        Duration secretEnd;
        String secretString = System.getenv(datasourceSecretName);
//        String secretString = System.getenv(datasourceSecretName);
        String secretString = "{\n" +
                "  \"server\": \"localhost\",\n" +
                "  \"port\": 3307,\n" +
                "  \"database\": \"import\",\n" +
                "  \"user\": \"root\",\n" +
                "  \"password\": \"SuperSecret\"\n" +
                "}";

        if (secretString != null) {
            log.debug("Located SQL creds as part of image");
            Gson gson = new Gson();
            secret = gson.fromJson(secretString, JsonObject.class);
        } else {
            log.debug("Getting creds from Secret Manager");
            secret = SecretUtils.getSecretJson(datasourceSecretName).getAsJsonObject();
            //secret = SecretUtils.getSecretJson(datasourceSecretName).getAsJsonObject();
            secret = SecretUtils.getSecretJson("impact-dev-partnerships",datasourceSecretName).getAsJsonObject();
        }

        secretEnd = Duration.between(secretBegin, Instant.now());
        log.debug(String.format("Retrieved secret in %s ms", secretEnd.toMillis()));

        return secret;
    }
}

